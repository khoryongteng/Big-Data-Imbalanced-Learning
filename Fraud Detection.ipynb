{"cells":[{"cell_type":"code","source":["# Installing Required Libraries\n%pip install pyspark\n%pip install pyspark_dist_explore\n%pip install imbalanced-learn\n%pip install pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b7cfab2-1128-4633-b962-8941d3d6d47a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Importing Required Libraries\n%matplotlib inline \nimport matplotlib.pyplot as plt\nimport pyspark.sql.functions as F\nfrom pyspark.sql import Row\nfrom pyspark.sql import SparkSession\n\n#Initializing Spark Session\nspark = SparkSession \\\n    .builder \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"10g\")\\\n    .appName(\"Fraud Detection BigD\") \\\n    .getOrCreate()\n\nsc = spark.sparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa202fac-886d-4c95-a5c7-5c4b59e17fc5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Loading Transaction and Identity Datasets as Spark Dataframes\nidentity = spark.read.csv(\"/mnt/bigd/train_identity.csv\", inferSchema =True, header=True,)\ntransaction = spark.read.csv(\"/mnt/bigd/train_transaction.csv\", inferSchema =True, header=True)\n\n# Joining Datasets on TransactionID\ndf = transaction.join(identity,['TransactionID'],how='left')\ndf.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d13cdb4-2074-43e8-9a33-3633417f100d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Counting Number of Rows and Columns in Dataset\nnum_rows = df.count()\nnum_columns = len(df.columns)\nprint(\"Number of rows : \", num_rows)\nprint(\"Number of columns : \", num_columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a99829e3-79b4-4d1a-8af1-7718011c02a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Showing top 5 Rows of the Dataset\ndf.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46421e21-e322-4b87-b6ab-21467730649a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Printing Schema\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b787eb4-dcc1-45b9-8040-6fffaabe7f5a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Data Exploration\nfrom pyspark_dist_explore import hist\nfrom pyspark.sql.types import StringType\n\n# Visualising Class Ratio\ndef check_classes(df) :\n  plt.figure()\n  df2 = df.withColumn('isFraud',df['isFraud'].cast(StringType()))\n  temp = df2.groupBy('isFraud').count()\n  hist_elect = temp.select('isFraud', \"count\").rdd.map(tuple).collect()\n  (x_values, y_values) = zip(*hist_elect)\n  plt.title('isFraud')\n  plt.bar(x_values, y_values)\n  plt.show()\n\ncheck_classes(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6f0d1fe-63e2-4aa8-9c64-b7284e1b40e5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Visualising Distribution of each Column\n# def check_features(df) :\n#   for data_type in df.drop(*['isFraud', 'TransactionID', 'TransactionDT']).schema.fields:\n#       if str(data_type.dataType) == \"IntegerType\" or str(data_type.dataType) == \"DoubleType\" :\n#         fig, ax = plt.subplots()\n#         ax.set_title(data_type.name)\n#         hist(ax, df.select(data_type.name), bins = 20, color=['red'])\n#       elif str(data_type.dataType) == \"StringType\" :\n#         plt.figure()\n#         temp = df.groupBy(data_type.name).count()\n#         hist_elect = temp.select(data_type.name, \"count\").rdd.map(tuple).collect()\n#         (x_values, y_values) = zip(*hist_elect)\n#         convert = lambda i : i or 'null'\n#         x_values = [convert(i) for i in x_values]\n#         plt.title(data_type.name)\n#         plt.bar(x_values, y_values)\n#         plt.show()\n\n# check_features(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"56721855-7266-41e0-8e8e-25b0caf8d937"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Check Number of NULL in Each Column\nfrom pyspark.sql.functions import col,isnan, when, count\nnull_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\nnull_counts.cache()\nnull_counts.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bca042e4-b1f8-4e7d-bd74-f0345c5d62a7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Drop Columns with >= 90% of NULL Values\ncolumn_to_drop = [k for k, number_of_null in null_counts.collect()[0].asDict().items() if number_of_null/num_rows >= 0.9]\nprint(\"Number fo columns to drop : \", len(column_to_drop))\nprint(\"Columns to drop : \", column_to_drop)\ndf_dropped = df.drop(*column_to_drop,'TransactionID','TransactionDT')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1c91358-0fcd-4151-8ca2-094c4582cfe4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Imputing all NULL Values in Categorical Columns with \"Unknown\"\ndf_categorical = df_dropped.na.fill(\"Unknown\")\ndf_categorical.cache()\ndf_categorical.show()\ndf.unpersist()\nnull_counts.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a33e381a-5a04-4681-b8a6-c7d3455adcef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#  Convert Categorical Variables to Numeric Variables\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\n\ncategorical_columns = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df_categorical) for column in list(categorical_columns) ]\npipeline = Pipeline(stages=indexers)\ndf_numerical = pipeline.fit(df_categorical).transform(df_categorical).drop(*categorical_columns)\ndf_numerical.cache()\ndf_numerical.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"013d0163-eca6-442f-9c04-fd05e6b01fe6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Imputing NULL Values in Numerical Columns with the Column Mean\nfrom pyspark.ml.feature import Imputer\nfrom collections import Counter\ncategorical_columns = ['ProductCD_index', 'card4_index', 'card6_index', 'P_emaildomain_index', 'R_emaildomain_index', 'M1_index', 'M2_index', 'M3_index', 'M4_index', 'M5_index', 'M6_index', 'M7_index', 'M8_index', 'M9_index', 'id_12_index', 'id_15_index', 'id_16_index', 'id_28_index', 'id_29_index', 'id_30_index', 'id_31_index', 'id_33_index', 'id_34_index', 'id_35_index', 'id_36_index', 'id_37_index', 'id_38_index', 'DeviceType_index', 'DeviceInfo_index','isFraud']\n\nnumerical_columns = list(set(df_numerical.columns) - set(categorical_columns))\n\nimputer = Imputer(\n  inputCols=numerical_columns,\n  outputCols=[\"{}_imputed\".format(c) for c in numerical_columns]\n).setStrategy(\"mean\")\n\ndf_imputed = imputer.fit(df_numerical).transform(df_numerical).drop(*numerical_columns)\ndf_imputed.cache()\ndf_imputed.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a71ff3d-051c-46eb-8141-6bca31cb7431"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Check if NULL values are All Imputed\ndf_imputed.select([count(when(col(c).isNull(), c)).alias(c) for c in df_imputed.columns]).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5983fec4-95d1-47db-a1b4-ec8c06e7b396"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Convert All Data to Float Except isFraud\ndf_float = df_imputed.select('isFraud', *(F.col(c).cast('float').alias(c) for c in df_imputed.columns[1:]))\ndf_float.cache()\ndf_float.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1b618e4-7871-425e-9fba-019abfc62491"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Unpersisting Preprocessing Cached Dataframes\ndf_categorical.unpersist()\ndf_numerical.unpersist()\ndf_imputed.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82e67d6e-68af-40ae-bdf1-cda240ab99bf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Splitting Dataset into Train and Test Sets\ndf_train, df_test = df_float.randomSplit([0.8,0.2])\ndf_train.cache()\ndf_test.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"460bdfb6-0985-4cb1-95b9-454dac9dd5e2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Vectorising Train and Test Sets\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\nfeature_cols = df_train.columns\nfeature_cols.remove('isFraud')\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n\ndf_train_vector = assembler.transform(df_train).select('features', 'isFraud')\ndf_train_vector.cache()\ndf_train_vector.show(5)\n\ndf_test_vector = assembler.transform(df_test).select('features', 'isFraud')\ndf_test_vector.cache()\ndf_test_vector.show(5)\n\ndf_float.unpersist()\ndf_train.unpersist()\ndf_test.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb9ad800-f99e-4696-9bb8-da9278f3fa48"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Scaling Train and Test Sets Before Application of PCA\nscaler = StandardScaler(\n    inputCol = 'features', \n    outputCol = 'scaledFeatures',\n    withMean = True,\n    withStd = True\n).fit(df_train_vector)\n\ndf_train_scaled = scaler.transform(df_train_vector).select('scaledFeatures', 'isFraud')\ndf_train_scaled.cache()\ndf_train_scaled.show(5)\n\ndf_test_scaled = scaler.transform(df_test_vector).select('scaledFeatures', 'isFraud')\ndf_test_scaled.cache()\ndf_test_scaled.show(5)\n\ndf_train_vector.unpersist()\ndf_test_vector.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c30e94b6-9a23-4015-80f3-6f4a4a59092d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Applying PCA to Train and Test Sets\nn_components = 40\npca = PCA(\n    k = n_components, \n    inputCol = 'scaledFeatures', \n    outputCol = 'pcaFeatures'\n).fit(df_train_scaled)\n\nprint('Explained Variance Ratio', sum(pca.explainedVariance.toArray()))\n\ndf_train_pca = pca.transform(df_train_scaled).select('pcaFeatures', 'isFraud')\ndf_train_pca.cache()\ndf_train_pca.show(5)\n\ndf_test_pca = pca.transform(df_test_scaled).select('pcaFeatures', 'isFraud')\ndf_test_pca.cache()\ndf_test_pca.show(5)\n\ndf_train_scaled.unpersist()\ndf_test_scaled.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61310576-dfed-42ef-98cb-ca320814b885"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Plotting Scatterplot of Principal Components\nimport numpy as np\nX_pca = df_train_pca.rdd.map(lambda row: row.pcaFeatures).collect()\nX_pca = np.array(X_pca)\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = 8, 6\nplt.rcParams['font.size'] = 12\n\ndef plot(X_pca, y):\n    markers = 's', 'x', 'o'\n    colors = list(plt.rcParams['axes.prop_cycle'])\n    target = np.unique(y)\n    for idx, (t, m) in enumerate(zip(target, markers)):\n        subset = X_pca[y == t]\n        plt.scatter(subset[:, 0], subset[:, 1], s = 50,\n                    c = colors[idx]['color'], label = t, marker = m)\n\n    plt.xlabel('PC 1')\n    plt.ylabel('PC 2')\n    plt.legend(loc = 'lower left')\n    plt.tight_layout()\n    plt.show()\n    \nplot(X_pca, df_train_pca.select(F.collect_list('isFraud')).first()[0])\n\n  \n# Plot bar graph\nplt.bar(list(range(1, 41)), pca.explainedVariance.toArray())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f78b9ba3-8718-4192-bc1f-7738c3b58e38"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Unvectorising Train and Test Sets\nfrom pyspark.ml.functions import vector_to_array\ndf_train = df_train_pca.withColumn('pcaFeatures', vector_to_array('pcaFeatures')).select([col(\"pcaFeatures\")[i] for i in range(40)] + ['isFraud'])\ndf_test = df_test_pca.withColumn('pcaFeatures', vector_to_array('pcaFeatures')).select([col(\"pcaFeatures\")[i] for i in range(40)] + ['isFraud'])\ndf_train.cache()\ndf_test.cache()\ncolumns = df_train.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19a0cfaa-910b-4362-8e6d-e652f1d5a9a7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Splitting Train Set into Fraud (Minority Class) and Not Fraud (Majority Class) Datasets\ntrain_fraud = df_train.filter(df_train.isFraud == 1)\ntrain_notfraud = df_train.filter(df_train.isFraud == 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d622f6c2-71ed-4171-91e8-07db7ce9cbf3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Partitioning Not Fraud (Majority Class) Dataset using RDD\n# Uncomment to Run Specific Number of Partitions\nnumpartitions = 11\n# numpartitions = 15\n# numpartitions = 19\n\ntrain_rdd = train_notfraud.rdd.repartition(numpartitions)\ntrain_rdd.cache()\ntest_rdd = df_test.rdd.repartition(10)\ntest_rdd.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19e93b1b-ae58-4ccd-99c8-44ae023ff143"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Broadcasting Fraud (Minority Class) Dataset\ntrain_fraud_bc = sc.broadcast(train_fraud.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1db74d57-5826-4134-abb1-bb0ea962cb53"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_train_pca.unpersist()\ndf_test_pca.unpersist()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"346a8171-cf32-457f-a1de-02d575bc94cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Function for Mapping each partition to a pandas dataset\ndef toPandas_partition(instances):\n  import pandas as pd\n  panda_df = pd.DataFrame(columns = columns)\n  for instance in instances:\n    panda_df = panda_df.append(instance.asDict(), ignore_index=True)\n  return [panda_df]\n\n#Function for Broadcasting a Copy of Fraud (Minority Class) to All Partitions\ndef broadcast_fraud_data(partition):\n  pd_train_fraud = toPandas_partition(train_fraud_bc.value)[0]\n  partition = partition.append(pd_train_fraud, ignore_index=False)\n  return partition\n\n#Function to Perform ADASYN on Each Partitions\ndef adasyn(partition):\n  import pandas as pd\n  from imblearn.over_sampling import ADASYN\n  ada = ADASYN()\n  X = partition.loc[:,'pcaFeatures[0]':'pcaFeatures[39]']\n  y = partition['isFraud']\n  X_resampled, y_resampled = ada.fit_resample(X, y)\n  partition = pd.concat([X_resampled, y_resampled], axis = 1)\n  return partition\n\n#Function to Build a GradientBoostingClassifier Model For Each Partition\ndef build_model_Boost(partition):\n  from sklearn.ensemble import GradientBoostingClassifier\n  X_train = partition.loc[:,'pcaFeatures[0]':'pcaFeatures[39]']\n  y_train = partition['isFraud']\n  cl = GradientBoostingClassifier()\n  model = cl.fit(X_train,y_train)\n  return model\n\n#Function to Build a SVC Model For Each Partition\ndef build_model_SVM(partition):\n  from sklearn.svm import SVC\n  X_train = partition.loc[:,'pcaFeatures[0]':'pcaFeatures[39]']\n  y_train = partition['isFraud']\n  cl = SVC()\n  model = cl.fit(X_train,y_train)\n  return model\n\n#Function to Build a GaussianNB Model For Each Partition\ndef build_model_GauNB(partition):\n  from sklearn.naive_bayes import GaussianNB\n  X_train = partition.loc[:,'pcaFeatures[0]':'pcaFeatures[39]']\n  y_train = partition['isFraud']\n  cl = GaussianNB()\n  model = cl.fit(X_train,y_train)\n  return model\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb9df85e-d810-4d34-8d0b-2e003f88c8ec"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Mapping Functions to RDD, Producing an Ensemble of Classifier Models\nmodels_Boost = train_rdd.mapPartitions(toPandas_partition).map(broadcast_fraud_data).map(adasyn).map(build_model_Boost).collect()\nmodels_SVM = train_rdd.mapPartitions(toPandas_partition).map(broadcast_fraud_data).map(adasyn).map(build_model_SVM).collect()\nmodels_GauNB = train_rdd.mapPartitions(toPandas_partition).map(broadcast_fraud_data).map(adasyn).map(build_model_GauNB).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9a82653-262d-4bed-a611-9addf6ab7d99"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Model Testing Functions\ndef test_model_Boost(partition):\n  predictions = []\n  X_test = partition.loc[:,'pcaFeatures[0]':'pcaFeatures[39]']\n  for m in models_Boost:\n      predictions.append(m.predict(X_test).tolist())\n  predictions = np.array(predictions, dtype=np.int32).transpose()\n  majority_pred = []\n  for i in predictions:\n    u, c = np.unique(i, return_counts = True)\n    majority_pred.append(u[c == c.max()][0])\n  return majority_pred\n\ndef test_model_SVM(partition):\n  predictions = []\n  X_test = partition.loc[:,'pcaFeatures[0]':'pcaFeatures[39]']\n  for m in models_SVM:\n      predictions.append(m.predict(X_test).tolist())\n  predictions = np.array(predictions, dtype=np.int32).transpose()\n  majority_pred = []\n  for i in predictions:\n    u, c = np.unique(i, return_counts = True)\n    majority_pred.append(u[c == c.max()][0])\n  return majority_pred\n\ndef test_model_GauNB(partition):\n  predictions = []\n  X_test = partition.loc[:,'pcaFeatures[0]':'pcaFeatures[39]']\n  for m in models_GauNB:\n      predictions.append(m.predict(X_test).tolist())\n  predictions = np.array(predictions, dtype=np.int32).transpose()\n  majority_pred = []\n  for i in predictions:\n    u, c = np.unique(i, return_counts = True)\n    majority_pred.append(u[c == c.max()][0])\n  return majority_pred"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dece250e-bd5a-48e2-8f32-c29c57f9899e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Testing Classifier Models\nresults_Boost = test_rdd.mapPartitions(toPandas_partition).flatMap(test_model_Boost).collect()\nresults_SVM = test_rdd.mapPartitions(toPandas_partition).flatMap(test_model_SVM).collect()\nresults_GauNB = test_rdd.mapPartitions(toPandas_partition).flatMap(test_model_GauNB).collect()\ny_test = df_test.select(\"isFraud\").collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78e415d3-4c4d-4817-af8b-617aa83003d0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Display Confusion Matrix and Classifier Performance Based on Accuracy, F1, Precision and Recall\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\ncon_matrix = confusion_matrix(y_test, results_Boost)\ntn, fp, fn, tp = confusion_matrix(y_test, results_Boost).ravel()\nprint(\"Boost\")\nprint(\"TN: \", tn, \"FP: \", fp, \"FN: \", fn, \"TP: \", tp)\nprint(\"Accuracy: \", accuracy_score(y_test, results_Boost))\nprint(\"F1: \", f1_score(y_test, results_Boost))\nprint(\"Precision: \", precision_score(y_test, results_Boost))\nprint(\"Recall: \", recall_score(y_test, results_Boost))\n\ncon_matrix = confusion_matrix(y_test, results_SVM)\ntn, fp, fn, tp = confusion_matrix(y_test, results_SVM).ravel()\nprint(\"SVM\")\nprint(\"TN: \", tn, \"FP: \", fp, \"FN: \", fn, \"TP: \", tp)\nprint(\"Accuracy: \", accuracy_score(y_test, results_SVM))\nprint(\"F1: \", f1_score(y_test, results_SVM))\nprint(\"Precision: \", precision_score(y_test, results_SVM))\nprint(\"Recall: \", recall_score(y_test, results_SVM))\n\ncon_matrix = confusion_matrix(y_test, results_GauNB)\ntn, fp, fn, tp = confusion_matrix(y_test, results_GauNB).ravel()\nprint(\"GauNB\")\nprint(\"TN: \", tn, \"FP: \", fp, \"FN: \", fn, \"TP: \", tp)\nprint(\"Accuracy: \", accuracy_score(y_test, results_GauNB))\nprint(\"F1: \", f1_score(y_test, results_GauNB))\nprint(\"Precision: \", precision_score(y_test, results_GauNB))\nprint(\"Recall: \", recall_score(y_test, results_GauNB))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ada2ad9e-c968-420d-ae86-2293f6b06919"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Fraud Detection","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3310348366446554}},"nbformat":4,"nbformat_minor":0}
